{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5t8xZ-ODE0D"
   },
   "source": [
    "## Shortlisting algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2fgAu18Voxy"
   },
   "source": [
    "### Introduction\n",
    "You cannot know which algorithm will work best on your dataset beforehand. You must use trial and error to discover a shortlist of algorithms that do well on your problem that you can then double down on and tune further. I call this process spot-checking.\n",
    "\n",
    "The question is not:\n",
    "\n",
    "  >*What algorithm should I use on my dataset?*\n",
    "\n",
    "Instead it is:\n",
    "\n",
    "  >*What algorithms should I spot-check on my dataset?*\n",
    "\n",
    "You can guess at what algorithms might do well on your dataset, and this can be a good starting point. I recommend trying a mixture of algorithms and see what is good at picking out the structure in your data. Below are some suggestions when spot-checking algorithms on your dataset:\n",
    "\n",
    "- Try a mixture of algorithm representations (e.g. instances and trees).\n",
    "- Try a mixture of learning algorithms (e.g. different algorithms for learning the same type of representation).\n",
    "- Try a mixture of modelling types (e.g. linear and nonlinear functions or parametric and non-parametric).\n",
    "\n",
    "We are going to take a look at four classification algorithms that you can spot-check on your dataset.\n",
    "\n",
    "- Logistic Regression\n",
    "- k-Nearest Neighbors.\n",
    "- Classification and Regression Trees (CART).\n",
    "- Support Vector Machines.\n",
    "\n",
    "We will then look at how you might go about tuning a model to find the best parameters. Finally, we will demonstrate how to save and load the best model for future use in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI9l_A_7YHaU"
   },
   "source": [
    "### Install Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2qZt927YH-R"
   },
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mWdjBZr2Cc7"
   },
   "source": [
    "### Stellar Classification Dataset (SDSS17)\n",
    "Astronomers use different colours of light to measure how bright stars and galaxies are, break their light apart to find out what they’re made of, and calculate how far and how fast they’re moving away. Altogether, this gives us a vast amount of information about the universe.\n",
    "\n",
    "The dataset we will use was derived from the Sloan Digital Sky Survey (SDSS), a long-term astronomical survey that maps the sky using a dedicated 2.5-meter wide-angle optical telescope located at Apache Point Observatory in New Mexico, USA. The SDSS17 dataset comes from the 17th data release of this project.\n",
    "\n",
    "Astronomers observe space using powerful telescopes and collect two main types of data:\n",
    "\n",
    "*1.) Photometric observations (brightness in different colours)*\n",
    "Imagine taking a photograph of the night sky, but instead of using just one colour, you take five different images using distinct colour filters. These filters are labelled:\n",
    "\n",
    "- `u` (ultraviolet) – captures the shortest wavelengths (invisible to our eyes)\n",
    "- `g` (green)\n",
    "- `r` (red)\n",
    "- `i` (infrared)\n",
    "- `z` (even deeper infrared)\n",
    "\n",
    "Each filter records how bright a star or galaxy appears in that portion of the light spectrum. This helps scientists determine:\n",
    "\n",
    "- The object’s temperature\n",
    "- Its composition (what it is made of)\n",
    "- How old it might be\n",
    "\n",
    "Think of it like looking at a fire through coloured glasses—you can learn a lot about the flame based on which glasses make it look brightest.\n",
    "\n",
    "*2.) Spectroscopic Observations (splitting light like a rainbow)*\n",
    "\n",
    "The second type of data is spectroscopic observations. This involves taking the light from a star or galaxy and passing it through a prism to separate it into its component colours. This “rainbow” of light is known as a spectrum. Why is this useful?\n",
    "\n",
    "Different elements (such as hydrogen, helium, etc.) leave unique fingerprints in the spectrum. These fingerprints reveal which elements are present, how hot the object is, and whether it is moving.\n",
    "\n",
    "The universe is expanding, and galaxies are moving away from us. When they do, their light becomes stretched—similar to how the pitch of a siren drops as an ambulance drives away.\n",
    "\n",
    "This stretching is called redshift, as the light shifts towards the red end of the spectrum. A higher redshift means the object is moving away faster, and often indicates it is further away. This helps astronomers understand the structure and history of the universe—how it has evolved over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YkOmCj0qwhL"
   },
   "source": [
    "### Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YujEB0GYffq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/martyn-harris-bbk/AppliedMachineLearning/refs/heads/main/data/star_classification.csv\")\n",
    "\n",
    "# Sample rows per class from the 'class' column (some models like SVM, take a long time to train)\n",
    "# You can use the full dataset by commenting out the line below if you have the hardware and time:\n",
    "df = df.groupby('class', group_keys=False).sample(n=1000, random_state=42)\n",
    "\n",
    "# Preview the balanced sample\n",
    "print(df['class'].value_counts())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gP_DumoURMYl"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWFl8yKdSklu"
   },
   "source": [
    "Let's look at the variables in the dataset that provide useful information about each object:\n",
    "\n",
    "| Column Name     | Description                                                                 |\n",
    "|------------------|-----------------------------------------------------------------------------|\n",
    "| obj_ID           | Unique ID for each object                                                   |\n",
    "| alpha            | Right ascension (RA) – celestial longitude in degrees                       |\n",
    "| delta            | Declination – celestial latitude in degrees                                 |\n",
    "| u                | Ultraviolet filter magnitude                                                |\n",
    "| g                | Green filter magnitude                                                      |\n",
    "| r                | Red filter magnitude                                                        |\n",
    "| i                | Near-infrared filter magnitude                                              |\n",
    "| z                | Infrared filter magnitude                                                   |\n",
    "| run_ID           | Observation run ID                                                          |\n",
    "| rereun_ID        | Data processing ID                                                          |\n",
    "| cam_col          | Camera column where the object was observed                                 |\n",
    "| field_ID         | ID of the field (region) of sky observed                                    |\n",
    "| spec_obj_ID      | Spectroscopic object ID                                                     |\n",
    "| class            | **Target column**: Object type – `GALAXY`, `STAR`, or `QSO`                |\n",
    "| redshift         | Redshift value – used to estimate distance and motion of the object         |\n",
    "| plate            | Spectroscopic plate ID                                                      |\n",
    "| MJD              | Modified Julian Date of the observation                                     |\n",
    "| fiber_ID         | Fiber ID used in spectroscopic observation                                  |\n",
    "| spec_class       | Spectral class (`GALAXY`, `QSO`, `STAR`, etc.)                              |\n",
    "\n",
    "### The target variable\n",
    "The column `class` is the label we want to predict. It tells us the type of astronomical object we're looking at. The values in this column include:\n",
    "\n",
    "- `GALAXY` – A massive collection of stars, gas, and dust bound together by gravity.\n",
    "- `QSO` (Quasi-Stellar Object or quasar) – Extremely bright and distant objects powered by black holes at the centre of galaxies.\n",
    "- `STAR` – A luminous sphere of plasma, like our Sun.\n",
    "\n",
    "In a classification task, the goal is to use the other data (features) to correctly assign new objects to one of these three categories.\n",
    "\n",
    "This means it's important to understand what you're trying to predict (the target) and which pieces of data might help you make that prediction.\n",
    "\n",
    "We can then select these good features, and preprocess them if necessary, to make them suitable for our machine learning models.\n",
    "\n",
    "#### Photometric magnitudes\n",
    "\n",
    "The columns `u`, `g`, `r`, `i`, `z` represent how bright an object appears in different parts of the light spectrum, from ultraviolet to infrared.\n",
    "\n",
    "Since galaxies, stars, and quasars have different light profiles, these magnitudes can be powerful indicators for classification. For example:\n",
    "\n",
    "- *Galaxies* have more balanced light across *all* bands.\n",
    "- *Quasars* tend to be brighter in `u` (ultraviolet).\n",
    "- *Stars* might appear brighter in the `g `and `r` bands.\n",
    "\n",
    "#### Redshift\n",
    "\n",
    "The column `redshift` measures how much the light from an object has been stretched due to the expansion of the universe. Higher redshift = further away and often implies the object is moving faster.\n",
    "\n",
    "Typically, we tend to observe that:\n",
    "\n",
    "- *Galaxies* have moderate redshifts.\n",
    "- *Quasars* tend to have very high redshifts (they're far away).\n",
    "- *Stars* have very low redshift values (they're within our own galaxy).\n",
    "\n",
    "So redshift is often a strong hint about the object’s identity (our target label)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Fdqbnc5fulj"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Now we understand which features we will use, let's preprocess the data to prepare it for feature selection.  The following columns don’t help the model learn the true patterns in the data (i.e., the science), and some could cause the model to latch onto artefacts or noise, resulting in poor generalisation. This because most of them refer to unique identifiers, so we will remove them:\n",
    "\n",
    "The goal is to train a model that learns the true physical characteristics of stars, galaxies, and quasars—not how the data was indexed or how the telescope happened to record them:\n",
    "\n",
    "| Column Name                | Reason for Removal                                                                                                                                              |\n",
    "|---------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `obj_ID`                  | Just a unique identifier (like a serial number); it has no relationship to the object's physical properties. Keeping it could confuse the model or introduce overfitting. |\n",
    "| `spec_obj_ID`             | Another unique ID, this time for the spectroscopic observation. Again, not useful for learning patterns.                                                        |\n",
    "| `run_ID`, `rerun_ID`      | These are technical details about the observation process—not about the object itself.                                                                           |\n",
    "| `field_ID`, `fiber_ID`, `plate` | These refer to the telescope hardware or sky region where the object was captured. While they might hold some observational bias, they don’t provide physical characteristics of the object. In general, we avoid training models on metadata unless there’s a strong justification. |\n",
    "| `MJD` |  Modified Julian Date represents the date of observation. It might correlate with some observational quirks, but it’s not related to whether an object is a star, galaxy, or quasar. |\n",
    "\n",
    "Let's start by removing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZMz0R15SnnG"
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\n",
    "    'obj_ID', 'spec_obj_ID', 'run_ID', 'rerun_ID',\n",
    "    'field_ID', 'fiber_ID', 'plate', 'MJD'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdXQIIQfSoJn"
   },
   "source": [
    "Let's also drop missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vlo8GbrSoXu"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nDayqTwS0Me"
   },
   "source": [
    "### Feature selection\n",
    "\n",
    "We now prepare the dataset for a classification task by selecting relevant features and encoding the target labels. We first define the input features (`u`, `g`, `r`, `i`, `z`, `redshift`) and the target column (`class`), then extract these columns from the DataFrame into `X` (features) and `Y` (labels).\n",
    "\n",
    "Since the target values are categorical (`'GALAXY'`, `'QSO'`, `'STAR'`), the code uses `LabelEncoder` from scikit-learn to convert these string labels into numeric values (e.g., 0, 1, 2), which are required for most machine learning models.\n",
    "\n",
    "Additionally, we create a dictionary (`label_mapping`) that shows how the original class names were mapped to numbers. This is helpful for interpreting model predictions later, as it allows you to translate encoded labels back to their original form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wB-h5u5zS12W"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Our features and target\n",
    "features = ['u', 'g', 'r', 'i', 'z', 'redshift']\n",
    "target = 'class'\n",
    "\n",
    "# Use the full dataset\n",
    "X = df[features]\n",
    "Y = df[target]\n",
    "\n",
    "# Encode the target labels (Galaxy, QSO, Star -> 0, 1, 2)\n",
    "label_encoder = LabelEncoder()\n",
    "Y= label_encoder.fit_transform(Y)\n",
    "\n",
    "# Optional: View the mapping of class names to encoded values\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "print(\"Label Mapping:\", label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wNFhZ8WjLkq"
   },
   "source": [
    "As we might have mentioned, many machine learning models — such as *K-Nearest Neighbours (KNN)*, *Support Vector Machines (SVM)*, and *Logistic Regression* — can be affected by the scale or range of the numbers in your dataset. For example, if one feature (like income) ranges from 0 to 100,000 and another (like age) ranges from 0 to 100, the model might give more importance to the larger numbers, even if they are not more important.\n",
    "\n",
    "To fix this, we use a *StandardScaler*, which makes all the features follow the same scale by removing the average (*mean*) from each feature and then scaling them so that they have a *standard deviation* of 1. In short, after this process, all features have a *mean of 0* and *equal spread*, so the model treats them fairly.\n",
    "\n",
    "Another helpful technique, especially when we're splitting data into *training* and *test* sets, is to use the *stratify* option — for example: `stratify=y_encoded`. This makes sure that the *class balance* (such as how many examples of each category you have, like \"cat\" and \"dog\" or \"spam\" and \"not spam\") is kept the same in both the training and test sets. This is important because if one set has mostly one class, the model may become biased or inaccurate.\n",
    "\n",
    "These steps help ensure that your model trains in a fair and balanced way, giving it a better chance of performing well on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c239exVStnH"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Scale the feature values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Fit to data, then transform it\n",
    "\n",
    "# Keep the process reproducible\n",
    "seed = 7\n",
    "\n",
    "# Split the dataset into training (80%) and test sets (20%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled, Y, test_size=0.2, random_state=seed, stratify=Y)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMlHKAU_Voxy"
   },
   "source": [
    "### Logistic Regression classification\n",
    "Imagine you're trying to figure out what type of space object something is—maybe a star, a galaxy, or a quasar—based on things like how bright it is and how far away it seems. You have a big list of known objects with this kind of information. Now, you want a method that looks at that data and learns patterns to help you make predictions for new objects.\n",
    "\n",
    "Logistic regression is a simple and popular method used to predict categories, especially when there are just two options—like whether an email is spam or not spam, or if a star is a galaxy or not. The name comes from its mathematical roots. It starts like a linear regression (which predicts numbers), but then it uses a special function called the logistic function to turn those numbers into probabilities between 0 and 1.\n",
    "\n",
    "It works by looking at numerical input data (like brightness or redshift) and tries to find patterns that separate one category from the other. It assumes the numbers follow a typical bell-curve shape (called a Gaussian distribution) but still works pretty well even if that's not perfectly true.\n",
    "\n",
    "While logistic regression is originally designed for binary classification (two classes), scikit-learn’s `LogisticRegression` automatically extends it to handle multiple classes. You can read the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">API Documentation</a> for more information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4O7pSWEVoxy"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "# Set up K-Fold cross-validation:\n",
    "# shuffle=True ensures the data is shuffled before splitting (important for randomness)\n",
    "kfold = KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "\n",
    "# Define the logistic regression model\n",
    "# solver='lbfgs' is a good choice for multinomial problems\n",
    "# max_iter=2000 allows more iterations in case the model takes longer to converge\n",
    "\n",
    "# If you get a warning, your model still produce an accuracy—but it might not\n",
    "# be as accurate or stable as it could be if it had fully converged.\n",
    "model = LogisticRegression(solver='saga', max_iter=2000)\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "# Print the average accuracy across all 10 folds\n",
    "print(f'k-Fold Accuracy: {results.mean()*100:.3f}% ({results.std()*100:.3f}%)')\n",
    "\n",
    "# Train the model on the full training data\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generate and print the classification report\n",
    "report = classification_report(Y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAlq2-gJVoxy"
   },
   "source": [
    "### k-Nearest Neighbors (KNN)\n",
    "\n",
    "KNN uses a distance metric to find the k most similar instances in\n",
    "the training data for a new instance and takes the mean outcome of the neighbors as the prediction.\n",
    "\n",
    "In other words, imagine you see a new star in the sky, and you want to know what type it is—a galaxy, a quasar, or just a star. You don’t have a formula to figure it out, but you do have a big notebook of space objects you’ve seen before, each labelled with what it is.\n",
    "\n",
    "What would you do? You might say \"Let me find the objects in my notebook that look most similar to this new one—and go with whatever type they are.” That’s exactly what K-Nearest Neighbours (KNN) does.\n",
    "\n",
    "When given a new object (like a star with certain brightness and redshift), KNN measures how similar it is to everything else in the training data. It looks for the K most similar examples (its \"nearest neighbours\"). Then it checks which class (e.g., STAR, GALAXY, QSO) is the most common among those neighbours. That’s the prediction!  If you're predicting numbers (like temperature), it might take the average instead.  \n",
    "\n",
    "You can construct a KNN model using the ```KNeighborsClassifier``` class. See the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">API documentation</a> for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgIjTg4-Voxy"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(f'k-Fold Accuracy: {results.mean()*100:.3f}% ({results.std()*100:.3f}%)')\n",
    "\n",
    "# Train the model on the full training data\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generate and print the classification report\n",
    "report = classification_report(Y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxCW-PhwVoxy"
   },
   "source": [
    "### Support Vector Machines (SVM)\n",
    "Imagine you're looking at a scatterplot of stars, galaxies, and quasars, based on their brightness and redshift. Your goal is to draw a line (or curve) that clearly separates these objects into distinct groups. This is exactly what Support Vector Machines (SVMs) aim to do - they try to find the best possible boundary that separates one type of object from another.\n",
    "\n",
    "More formally, SVMs are designed to separate two classes by finding a line (in 2D) or a hyperplane (in higher dimensions) that leaves the widest possible gap between the classes. The most important data points in this process are the ones closest to the boundary—these are called support vectors. They directly influence where the dividing line is drawn.  SVMs have also been extended to handle multiple classes, like those in the SDSS dataset.\n",
    "\n",
    "A key strength of SVMs is their use of kernel functions, which allow them to draw curved boundaries instead of just straight lines. By default, SVM uses a kernel called the *Radial Basis Function (RBF)*, which works well for many real-world problems. *RBF* measures the similarity between points. It gives a high value when two points are close together and a low value when they are far apart. The idea is that nearby data points should have more influence on each other than distant ones.\n",
    "\n",
    "You can build an SVM model in scikit-learn using the SVC class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DDmUsg3Voxy"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(f'k-Fold Accuracy: {results.mean()*100:.3f}% ({results.std()*100:.3f}%)')\n",
    "\n",
    "# Train the model on the full training data\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generate and print the classification report\n",
    "report = classification_report(Y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcJAVaefVoxz"
   },
   "source": [
    "### Classification and Regression Trees (CART or just decision trees)\n",
    "\n",
    "Decision trees work a bit like playing 20 questions—they try to ask the best possible question at each step to help sort data into the right categories. At every point (called a *node*), the algorithm chooses a feature (like brightness or redshift) and decides how to split the data based on that feature.\n",
    "\n",
    "Choosing the right feature to split on is *crucial*. The goal is to pick the one that does the best job of dividing the data into clean groups. This is achieved by the algorithm trying all features, whereupon it chooses the ones that give the most *information gain* (for classification) or reduces uncertainty or \"impurity\" the most.\n",
    "\n",
    "The metric used depends on the type of decision tree:\n",
    "- *Gini impurity* (used in CART)\n",
    "- *Entropy/information gain* (used in ID3 and C4.5)\n",
    "- *Variance reduction* (for regression trees)\n",
    "\n",
    "For categorical features, the tree splits the data based on different class labels. For numerical features, it looks for the best cutoff value (like \"is redshift > 0.5?\") to divide the data.\n",
    "\n",
    "This process continues until the tree reaches a stopping point, such as:\n",
    "- All remaining data belongs to one class (a \"pure\" leaf),\n",
    "- The tree has reached a *maximum depth* (to avoid getting too complicated),\n",
    "- Or when there are not enough samples left to make another meaningful split.\n",
    "\n",
    "After the tree is built, we can make it simpler and more reliable by using *pruning*. This means trimming off parts of the tree that do not help much with predictions. Techniques like cost-complexity pruning or *reduced-error* pruning help prevent the model from overfitting the training data—making it better at predicting new, unseen examples.\n",
    "\n",
    "You can create a decision tree model using scikit-learn’s `DecisionTreeClassifier` class.  You can find more details in the [API documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). Here’s an example of how we build one on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lu0pLq6KVoxz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Fit a model\n",
    "model = DecisionTreeClassifier(criterion = \"gini\", random_state=seed)\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(f'k-Fold Accuracy: {results.mean()*100:.3f}% ({results.std()*100:.3f}%)')\n",
    "\n",
    "# Train the model on the full training data\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generate and print the classification report\n",
    "report = classification_report(Y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fAXfglMVoxz"
   },
   "source": [
    "### Decision trees and overfitting\n",
    "Decision trees can sometimes become too complex, trying to fit every little detail in the training data—even the noise or outliers.\n",
    "\n",
    "When this happens, the tree performs well on training data, but struggles to make accurate predictions on new, unseen data. This problem is known as *overfitting*.\n",
    "\n",
    "To prevent overfitting, it's important to limit the complexity of the tree. We will focus on pruning. Pruning a decision tree is a simple and effective way to reduce overfitting, and we can simply *prune* the tree by limiting its size or shape using a few key parameters. Some of the most commonly used parameters for pruning include:\n",
    "\n",
    "- `max_leaf_nodes`: Limits the number of leaf nodes in the tree. Fewer leaf nodes generally mean a simpler tree.\n",
    "- `min_samples_leaf`: Sets the minimum number of samples required to be at a leaf node. Helps avoid tiny splits that are not meaningful.\n",
    "- `max_depth`: Limits how deep the tree can go, controlling how many decisions the model can make in sequence.\n",
    "\n",
    "In the example below, we apply these pruning parameters to a `DecisionTreeClassifier`, and evaluate the pruned model using cross-validation and our trusty classification report:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNjFPv_ZVoxz"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "# Define a pruned decision tree model\n",
    "model = DecisionTreeClassifier(\n",
    "    criterion=\"gini\",           # Use Gini impurity to measure splits\n",
    "    splitter=\"random\",          # Randomly choose features for splitting (adds randomness)\n",
    "    max_leaf_nodes=10,          # Limit the number of leaf nodes (controls tree size)\n",
    "    min_samples_leaf=5,         # Require at least 5 samples in each leaf node\n",
    "    max_depth=5,                # Limit tree depth to 5 levels\n",
    "    random_state=seed           # Set seed for reproducibility\n",
    ")\n",
    "\n",
    "# Evaluate the model using k-fold cross-validation\n",
    "results = cross_val_score(model, X_train, Y_train, cv=kfold)\n",
    "\n",
    "# Print mean accuracy and standard deviation from cross-validation\n",
    "print(f'k-Fold Accuracy: {results.mean() * 100:.3f}% ({results.std() * 100:.3f}%)')\n",
    "\n",
    "# Train the model on the full training set\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generate a classification report showing precision, recall, F1-score, and support\n",
    "report = classification_report(Y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCKFICZ9RMYn"
   },
   "source": [
    "This approach demonstrates how adjusting just a few parameters can result in a simpler, more generalisable decision tree that performs well on unseen data. You can experiment with different values to see how they affect accuracy and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obQu96s9Vox0"
   },
   "source": [
    "### Comparing Machine Learning algorithms\n",
    "When working on a machine learning project, it’s common to build and test several different models. Each algorithm has its own strengths and weaknesses, and their performance can vary depending on the dataset. To choose the best one, we need a fair and consistent way to compare them.\n",
    "\n",
    "One effective approach is to use *cross-validation*, which gives you an estimate of how well each model is likely to perform on new, unseen data. These estimates help you decide which models are worth keeping and fine-tuning.\n",
    "\n",
    "Just like it’s helpful to visualise your dataset from different angles, it’s also important to visualise model performance from multiple perspectives. You can use graphs and plots to compare things like:\n",
    "- Average accuracy\n",
    "- Variability (spread or consistency)\n",
    "- Distribution of scores across different folds\n",
    "\n",
    "This gives you a clearer picture of which models are both accurate and reliable.\n",
    "\n",
    "To ensure a fair comparison, every model should be tested in exactly the same way—using the same data splits, same evaluation method, and same random seed. This ensures that differences in performance come from the models themselves, not from how they were tested.\n",
    "\n",
    "In the example below, six popular classification algorithms are compared using *10-fold cross-validation*, with a consistent test setup (we have explored some of these before):\n",
    "\n",
    "- Logistic Regression  \n",
    "- Linear Discriminant Analysis  \n",
    "- k-Nearest Neighbours (KNN)  \n",
    "- Decision Trees (CART)  \n",
    "- Naive Bayes  \n",
    "- Support Vector Machines (SVM)\n",
    "\n",
    "Each algorithm is assigned a short label to make it easier to summarise and visualise results later on. Creating models and storing them in a dictionary or list can really help test a range of models in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Q_dOn9yVox0"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in models:\n",
    "   kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "   cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\n",
    "   results.append(cv_results)\n",
    "\n",
    "   names.append(name)\n",
    "\n",
    "   msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\n",
    "   print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwDs6oXRRMYn"
   },
   "source": [
    "Looking at just the average accuracy, allows us to pick a smaller selection of models to work with and improve if they already show good performance from this initial investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJPBIwolO9dI"
   },
   "source": [
    "### Tuning models\n",
    "\n",
    "Tuning a model means making small adjustments to improve how well it performs. It’s one of the final steps in building a machine learning system, just before deciding that the model is ready to use.\n",
    "\n",
    "This process is often called *hyperparameter tuning* (or *hyperparameter optimisation*). The word *hyperparameter* refers to the settings or choices you make before the model starts learning — for example, how many neighbours to use in KNN, or how deep a decision tree can go. These are different from *parameters*, which are the values the model learns by itself from the data (like the weights in linear regression).\n",
    "\n",
    "When we say *optimisation*, we use it to refer to the fact that because tuning is like a search — you're trying to find the best combination of settings for your model, like searching for the right ingredients in a recipe to get the best result.\n",
    "\n",
    "#### Grid search\n",
    "The first approach we can try is *Grid search*, which is a methodical way to test different combinations of settings (hyperparameters). You first define a list of possible values for each setting, and the algorithm will try out *every* possible combination of those values to see which performs best.\n",
    "\n",
    "You can do this in code using the `GridSearchCV` class from scikit-learn. This class runs all the combinations, checks the performance, and shows you the best setup. See the [API Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to learn how to use it in more detail.\n",
    "\n",
    "Here we will use it to help prune a decision tree, by picking the best value for `max_depth`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPQMlb3NPAZf"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# The varying max_depth parameters to test and choose the best from\n",
    "vals = numpy.array([12, 10, 8, 6, 4])\n",
    "\n",
    "param_grid = dict(max_depth=vals) # The key is the parameter name\n",
    "\n",
    "model = DecisionTreeClassifier(\n",
    "    criterion=\"gini\",           # Use Gini impurity to measure splits\n",
    "    splitter=\"random\",          # Randomly choose features for splitting (adds randomness)\n",
    "    random_state=seed           # Set seed for reproducibility\n",
    ")\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Recommended max_depth value\", grid.best_estimator_.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVUmbpA2PB73"
   },
   "source": [
    "### Random search\n",
    "\n",
    "Random search is another way to tune a model by trying out different settings (called *hyperparameters*) — but instead of trying *every* possible combination like grid search, it picks them *randomly*. Think of it like reaching into a bag of possibilities and pulling out a few combinations to test, rather than checking them all. This can be much faster, especially when there are a lot of combinations, and it often still finds a very good result.\n",
    "\n",
    "In random search, values for each hyperparameter are picked randomly from a range you specify (usually using a *uniform distribution*, which means all values in the range have an equal chance of being chosen). The model is built and tested for each combination, just like in grid search.\n",
    "\n",
    "To do this in Python, you can use the `RandomizedSearchCV` class from scikit-learn. It handles the random selection, training, and evaluation for you. See the [API Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) for details on how to use it.\n",
    "\n",
    "In the example below, we explore a wide variety of settings without needing to try every single possible one. Again, we will tune our decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BI5LZlWLPCF_"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Suppress convergence warnings - not all parameters tested will be sensible\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Define integer-based parameter grid\n",
    "param_grid = {\n",
    "    'C': randint(1, 100),             # Inverse regularisation strength (converted to float in model)\n",
    "    'max_iter': randint(100, 2000),   # Number of iterations\n",
    "}\n",
    "model = LogisticRegression(solver='saga', random_state=seed)\n",
    "\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100, random_state=seed)\n",
    "rsearch.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Best score:\", rsearch.best_score_)\n",
    "print(\"Recommended C:\", rsearch.best_estimator_.C)\n",
    "print(\"Recommended max_iter:\", rsearch.best_estimator_.max_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2K3jlyvPK7P"
   },
   "source": [
    "Both tuning approaches narrow down the search for suitable parameters leaving you to experiment and judge from the measures of accuracy you get from the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bN79-SUGPPjn"
   },
   "source": [
    "### Saving and loading machine learning models\n",
    "\n",
    "Once you've trained a machine learning model and found one that performs well, it’s a good idea to save it so you can use it later — either in another script, in production, or simply to avoid retraining every time. This process is useful for sharing models or deploying them in real-world applications.\n",
    "\n",
    "In Python, a common way to save a model is by using a built-in tool called *pickle*. Pickle is a standard Python module that allows you to *serialise* objects. *Serialising* means converting the model into a format that can be stored in a file. Later, you can *deserialise* the file, which means loading the model back into memory exactly as it was.\n",
    "\n",
    "This is especially useful in machine learning, where training a model can take time. Saving the model once lets you skip that step when you want to make new predictions later. You can find more details in the [Pickle API documentation](https://docs.python.org/2/library/pickle.html), but the basic process is very straightforward. The example below shows how to:\n",
    "- Train a *logistic regression* model on a dataset.\n",
    "- Save the trained model to a file using *pickle*.\n",
    "- Load the model back from the file.\n",
    "- Use the loaded model to make predictions on a separate *test set*.\n",
    "\n",
    "This workflow helps you build once, reuse many times — an essential step when moving from experimentation to production or deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vh1AXX6oPRH_"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = './finalised_model.sav'\n",
    "\n",
    "print(\"Saving\", filename)\n",
    "dump(model, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "# some time later...\n",
    "\n",
    "\n",
    "# load the model from disk\n",
    "print(\"Loading\", filename)\n",
    "loaded_model = load(open(filename, 'rb'))\n",
    "\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkn-0bPxfHA2"
   },
   "source": [
    "### What have we learnt?\n",
    "\n",
    "We explored the process of building, evaluating, and comparing machine learning models using the SDSS17 astronomical dataset. Along the way, we broke down key concepts into clear, beginner-friendly explanations to help build a strong foundation in both data science and astronomy.\n",
    "\n",
    "We started by understanding the structure of the dataset, including features like photometric magnitudes and redshift, and why it's important to remove irrelevant columns (such as object IDs and technical metadata) during preprocessing. We then saw how to prepare the data using techniques like *label encoding*, *feature scaling*, and *train-test splitting*.\n",
    "\n",
    "From there, we explored several popular machine learning algorithms:\n",
    "\n",
    "- *Logistic Regression*: A model that estimates probabilities and is well-suited for classification, including multi-class problems when configured properly.\n",
    "- *K-Nearest Neighbours (KNN)*: A simple, intuitive model that predicts labels based on the most similar examples in the training data.\n",
    "- *Support Vector Machines (SVM)*: A powerful classifier that finds the optimal boundary between classes, with the ability to handle complex patterns using kernels.\n",
    "- *Decision Trees*: Models that split the data based on the most informative features, but can overfit if not properly controlled with techniques like max depth and pruning.\n",
    "\n",
    "We also discussed how to avoid overfitting and why it's important to evaluate models fairly and consistently, using tools like *cross-validation*. Finally, we looked at how to compare multiple models side by side and inspect their accuracy and variability to select the best-performing ones for the task.\n",
    "\n",
    "We demonstrated had to tune your chosen model to find the best parameters, and how to then save and load the model for future use. You should hopefully now feel better equipped to build robust machine learning solutions—even for complex datasets like those in astronomy."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1TZOGBDd77wWCfuF6mYryjhND98i1aXEO",
     "timestamp": 1743071059761
    },
    {
     "file_id": "1_uLcq3zY_iwnlBWi__SSHsJlZ3hFIwqJ",
     "timestamp": 1742826081387
    },
    {
     "file_id": "1mIc1vhK2-W4rJBFpJhxp3c7kI7nDEmuv",
     "timestamp": 1742822947450
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
