{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qxojpf-eVox0"
   },
   "source": [
    "## Pipelining\n",
    "### Introduction\n",
    "In applied machine learning, there are well-established workflows designed to tackle common issues—one of the most critical being data leakage. Data leakage happens when information from the test set influences the training process, leading to overly optimistic and misleading evaluation results.\n",
    "\n",
    "One of the most common ways this occurs is during data preparation. For instance, if you apply scaling or normalisation to the entire dataset before splitting into train and test sets, the model unintentionally gains information about the test data. To prevent this, it’s essential to build a robust evaluation setup where training and test data remain strictly separated, including during preprocessing.\n",
    "\n",
    "Scikit-learn’s Pipeline utility helps automate and streamline this process by chaining together preprocessing steps (like imputation, encoding, and scaling) with the modelling stage. Each step is applied in the correct order and only within the appropriate training folds, ensuring a clean and fair evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOVSjHOps1Ob"
   },
   "source": [
    "### Install Python libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8471,
     "status": "ok",
     "timestamp": 1746016170637,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "L2qZt927YH-R",
    "outputId": "20189345-c557-4aa9-f4c6-dda4b897efe0"
   },
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toD_g1YqDUt2"
   },
   "source": [
    "### Adult Income dataset\n",
    "\n",
    "The Adult Income dataset (also known as the *Census Income* dataset) was collected from the *1994 U.S. Census*. It is commonly used in machine learning for *binary classification* tasks, where the goal is to predict whether a person earns more than $50,000 per year based on demographic and employment-related attributes. This dataset is widely used for:\n",
    "- Benchmarking classification algorithms\n",
    "- Learning preprocessing pipelines (handling categorical & numerical data)\n",
    "- Studying model bias and fairness (e.g., income by gender or race)\n",
    "- Feature engineering practice\n",
    "\n",
    "The original purpose of this dataset was to analyse demographic and employment-related factors and predict whether an individual's income exceeds $50,000 per year—a threshold relevant for economic and social research.  This kind of analysis is useful for:\n",
    "\n",
    "- Economic planning and policy\n",
    "- Employment and wage studies\n",
    "- Social inequality research\n",
    "- Targeted surveys\n",
    "\n",
    "Each row represents one adult individual, with 14 input features and one target label (`income`):\n",
    "\n",
    "- <=50K: Income less than or equal to $50,000 a year\n",
    "\n",
    "- \\>50K: Income greater than $50,000 a year\n",
    "\n",
    "The training and test datasets are provided separately. The test set contains new individuals, which makes it suitable for model evaluation on new and unseen data. Here is an overciew of the features:\n",
    "\n",
    "| Feature           | Description                                                     |\n",
    "|-------------------|-----------------------------------------------------------------|\n",
    "| `age`             | Age of the individual                                           |\n",
    "| `workclass`       | Type of employment (e.g., Private, Self-emp, Government)        |\n",
    "| `fnlwgt`          | Final weight – a sampling weight used by the Census             |\n",
    "| `education`       | Highest level of education completed (e.g., Bachelors, HS-grad) |\n",
    "| `education_num`   | Numeric encoding of education level                             |\n",
    "| `marital_status`  | Marital status (e.g., Married, Single)                          |\n",
    "| `occupation`      | Job type (e.g., Tech-support, Sales, Craft-repair)              |\n",
    "| `relationship`    | Family role (e.g., Husband, Not-in-family)                      |\n",
    "| `race`            | Race of the individual                                          |\n",
    "| `sex`             | Gender of the individual                                        |\n",
    "| `capital_gain`    | Income from capital gains                                       |\n",
    "| `capital_loss`    | Losses from capital                                             |\n",
    "| `hours_per_week`  | Number of hours worked per week                                 |\n",
    "| `native_country`  | Country of origin                                               |\n",
    "| `income`          | *Target variable* – `<=50K` or `>50K`                         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0WFop3hs5ZG"
   },
   "source": [
    "### Loading the data\n",
    "\n",
    "You can load the dataset using `pandas` from the UCI repository and split it into training and test sets for modelling and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4755,
     "status": "ok",
     "timestamp": 1746016175396,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "qiJBjm84s5ld",
    "outputId": "4dc40f50-b713-4443-b5fa-37d5a32b743b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Column names from the dataset documentation\n",
    "column_names = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
    "    'hours_per_week', 'native_country', 'income'\n",
    "]\n",
    "\n",
    "# Load the train and test data\n",
    "url_train = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "\n",
    "df_train = pd.read_csv(url_train, header=None, names=column_names, na_values=' ?', skipinitialspace=True)\n",
    "\n",
    "url_test = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "\n",
    "df_test = pd.read_csv(url_test, skiprows=1, header=None, names=column_names, na_values=' ?', skipinitialspace=True)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "# Remove trailing period in target\n",
    "df_test['income'] = df_test['income'].str.replace('.', '', regex=False)\n",
    "\n",
    "# Separate features and target for training data\n",
    "X_train = df_train.drop('income', axis=1)\n",
    "Y_train = df_train['income']\n",
    "\n",
    "X_test = df_test.drop('income', axis=1)\n",
    "Y_test = df_test['income']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIWKVrG9Ygcz"
   },
   "source": [
    "Since we’re loading the training and test sets separately (from the UCI Adult dataset), there's no need to use `train_test_split` — we already have separate datasets (`df_train` and `df_test`).\n",
    "\n",
    "For some datasets you will often find that this step has been prepared for you with ample volumes of training and test data. Given the amount of data (32K+ rows), you can of course download and perform test-train split as usual using just the training data. More data is usually a good thing when it comes to more sophisticated models though.\n",
    "\n",
    "We will take a smaller sample of the data for training (this is a big dataset), you can increase this if you wish based on your hardware and the time you have available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746016175399,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "BqPfyDQq3udZ"
   },
   "outputs": [],
   "source": [
    "# Define sample size\n",
    "train_sample_size = 500\n",
    "test_sample_size = 200\n",
    "\n",
    "seed = 7\n",
    "\n",
    "# Sample from training and test sets (set random_state for reproducibility)\n",
    "X_train = X_train.sample(n=train_sample_size, random_state=seed)\n",
    "Y_train = Y_train.loc[X_train.index]  # Ensure labels match sampled features\n",
    "\n",
    "X_test = X_test.sample(n=test_sample_size, random_state=seed)\n",
    "Y_test = Y_test.loc[X_test.index]     # Match test labels to test features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l-eIO41o2H5"
   },
   "source": [
    "In the above, we randomly select a subset of rows from both training and test sets. We also ensure the corresponding labels (`Y_train`, `Y_test`) are aligned correctly using `.loc[]` with the sampled indices. Let's review our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1746016175557,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "1bzROZLf1MbT",
    "outputId": "d8c13a5c-e188-4585-e04a-37b7167e3855"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhozxEyQHWHT"
   },
   "source": [
    "Let's also group our columns of data based on whether they are numeric (and will require scaling), or text (requiring categorisation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1746016175559,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "XC74RAUOImBb"
   },
   "outputs": [],
   "source": [
    "# Separate feature types\n",
    "numerical_features = [\n",
    "    'age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'workclass', 'education', 'marital_status', 'occupation',\n",
    "    'relationship', 'race', 'sex', 'native_country'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkvwM023Vox0"
   },
   "source": [
    "### Working with pipelines - data preparation\n",
    "Pipelines help you prevent data leakage in your test harness by ensuring that data preparation like standardisation is constrained to each fold of your cross-validation procedure.  We will create a pipeline that standardises the data then creates a model. The pipeline is defined with two steps:\n",
    "\n",
    "1. Standardise the data.\n",
    "2. Create the model - a Linear Discriminant Analysis model.\n",
    "\n",
    "The pipeline is then evaluated using 10-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3798,
     "status": "ok",
     "timestamp": 1746016179353,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "-_iygRzEVox0",
    "outputId": "8b6bdbf7-16f5-40e6-8729-0b73bff9bd9e"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Select only numerical columns from the training and test sets\n",
    "X_train_num = X_train[numerical_features]\n",
    "X_test_num = X_test[numerical_features]\n",
    "\n",
    "# Create pipeline with standardisation and LDA model\n",
    "estimators = [\n",
    "    ('standardise', StandardScaler()),\n",
    "    ('lda', LinearDiscriminantAnalysis())\n",
    "]\n",
    "\n",
    "# The estimators are passed to the pipeline\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "seed = 7\n",
    "\n",
    "# Set up 10-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "# Evaluate using cross-validation\n",
    "results = cross_val_score(model, X_train_num, Y_train, cv=kfold)\n",
    "\n",
    "# Evaluate final model on test set\n",
    "model.fit(X_train_num, Y_train)\n",
    "\n",
    "test_accuracy = model.score(X_test_num, Y_test)\n",
    "\n",
    "# Show results\n",
    "print(\"Cross-validated Training Accuracy: {:.4f}\".format(results.mean()))\n",
    "print(\"Test Accuracy: {:.4f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhoZT6e5OAv4"
   },
   "source": [
    "This illustrates the basic pipeline framework used in machine learning workflows. However, for our dataset, we need to extend the pipeline to handle real-world challenges in the data more effectively. The dataset contains a mix of *numerical* and *categorical* features, and it may also include *missing values*.\n",
    "\n",
    "These issues are common in applied machine learning tasks, and they need to be addressed before training a model. If left untreated, they can lead to poor performance or even errors during training.  To prepare the dataset properly, we need to perform several key preprocessing steps:\n",
    "\n",
    "- *Imputing missing values*: Filling in missing or null entries so the model can be trained without errors. This might involve replacing missing values with the mean (for numbers) or the most frequent value (for categories).\n",
    "- *Encoding categorical variables*: Converting non-numeric columns (like strings or labels) into numeric form, so they can be understood by algorithms. Common techniques include one-hot encoding or ordinal encoding.\n",
    "- *Scaling numerical features*: Adjusting the scale of numeric values so that features with large ranges (like income or age) don’t dominate features with smaller ranges. This is especially important for algorithms sensitive to feature magnitude, such as KNN or logistic regression.\n",
    "\n",
    "To organise all of these steps clearly and efficiently, we can use a `ColumnTransformer` within a `Pipeline`. The `ColumnTransformer` allows us to apply different preprocessing strategies to different subsets of columns — for example, scaling only the numeric ones and encoding only the categorical ones. This ensures that each type of data is treated appropriately in a single, unified pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2957,
     "status": "ok",
     "timestamp": 1746016182312,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "FyxAdfGVHWnD",
    "outputId": "009b27ab-bf2d-4d5f-a22a-e762c5efbb73"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define preprocessing pipeline for numerical features:\n",
    "# 1) Impute missing values using the median\n",
    "# 2) Scale features to have mean 0 and standard deviation 1\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define preprocessing pipeline for categorical features:\n",
    "# 1) Impute missing values using the most frequent category\n",
    "# 2) Convert categories into one-hot encoded binary columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine the two transformers using a ColumnTransformer:\n",
    "# This applies the appropriate pipeline to each column type\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_features),     # Apply to numeric columns\n",
    "    ('cat', categorical_transformer, categorical_features)  # Apply to categorical columns\n",
    "])\n",
    "\n",
    "# Create a full pipeline:\n",
    "# 1) Preprocess the data\n",
    "# 2) Fit a logistic regression model on the transformed data\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))  # Increased max_iter to ensure convergence\n",
    "])\n",
    "\n",
    "# Train the pipeline on the training set\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Set up 10-fold cross-validation to evaluate training performance\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(model, X_train, Y_train, cv=kfold)\n",
    "\n",
    "# Evaluate model accuracy on training and test sets\n",
    "print(\"Cross-validated Training Accuracy: {:.4f}\".format(results.mean()))\n",
    "print(\"Test Accuracy: {:.4f}\".format(test_accuracy))  # test_accuracy must be defined elsewhere\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQNBiwRvVox0"
   },
   "source": [
    "### Working with pipelines – feature extraction\n",
    "\n",
    "*Feature extraction* is the process of transforming raw input data into new features that can improve model performance. However, like other steps in data preparation, feature extraction must be done *carefully* to avoid *data leakage*. This means that all feature extraction should be performed only on the training data, never on the test set or across folds during cross-validation — otherwise, information from the test data can \"leak\" into training, leading to overly optimistic results.\n",
    "\n",
    "To help with this, scikit-learn offers a powerful tool called `FeatureUnion`. This allows us to combine multiple feature extraction steps into a single, unified set of features, which are then passed to the model. Importantly, when used within a pipeline, all transformations and the union of features occur within each fold of cross-validation, ensuring no leakage.\n",
    "\n",
    "In the example below, we build a pipeline that includes two parallel feature extraction techniques and a classifier, combined in four main steps:\n",
    "\n",
    "1. *Feature Extraction with Principal Component Analysis (PCA)*: Reduces the dimensionality of the data to three components that capture the most variance.\n",
    "2. *Feature Extraction with Statistical Selection*: Selects the six most relevant features using a univariate statistical test (e.g. ANOVA F-test).\n",
    "3. *Feature Union*: Merges the outputs from the two feature extraction techniques into a single feature set.\n",
    "4. *Logistic Regression Model*: Trains a logistic regression classifier on the combined features.\n",
    "\n",
    "This entire process is wrapped inside a pipeline and evaluated using 10-fold cross-validation. A key takeaway is that `FeatureUnion` itself acts like a *mini-pipeline*, and becomes a *single step* in the final pipeline that leads to model training.\n",
    "\n",
    "This shows how you can nest pipelines inside pipelines — a flexible and clean way to manage complex preprocessing and modelling workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1689,
     "status": "ok",
     "timestamp": 1746016183998,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "hszxjZUoVox0",
    "outputId": "b7ddd80d-c2fc-4047-e981-ef1b6e702f28"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Define preprocessing for numerical features:\n",
    "# 1) Impute missing values using the median\n",
    "# 2) Scale features to standard normal distribution (mean=0, std=1)\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define preprocessing for categorical features:\n",
    "# 1) Impute missing values with the most frequent value\n",
    "# 2) One-hot encode categories (convert to binary columns)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine both transformers into a single preprocessor:\n",
    "# This applies the right transformation to each feature type\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Define feature extraction steps:\n",
    "# 1) PCA reduces the feature space to 3 dimensions (captures most variance)\n",
    "# 2) SelectKBest selects the 6 most statistically relevant features\n",
    "# Both are applied in parallel and their results combined\n",
    "feature_union = FeatureUnion([\n",
    "    ('pca', PCA(n_components=3)),\n",
    "    ('select_best', SelectKBest(k=6))\n",
    "])\n",
    "\n",
    "# Define full pipeline:\n",
    "# 1) Preprocess the data\n",
    "# 2) Extract features using both PCA and SelectKBest\n",
    "# 3) Train a logistic regression model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('features', feature_union),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Set up k-fold cross-validation (10 folds, shuffled, with fixed seed)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "# Evaluate model performance using cross-validation on training data\n",
    "results = cross_val_score(model, X_train, Y_train, cv=kfold)\n",
    "\n",
    "# Print training and test accuracy\n",
    "print(\"Cross-validated Training Accuracy: {:.4f}\".format(results.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGWxkCy32P-S"
   },
   "source": [
    "Using a pipeline helps prevent *data leakage*, which is a common pitfall in machine learning. By wrapping both the *preprocessing steps* and the *model* inside a single pipeline, we ensure that any transformations (like scaling or encoding) are applied **only** to the training data within each fold of cross-validation. This means the model doesn’t accidentally learn patterns from the test set, leading to more honest and reliable results.\n",
    "\n",
    "Pipelines also result in cleaner and more reusable code. They make your workflow modular — each step is clearly defined and easy to swap, tune, or extend. This is especially helpful in larger projects or when collaborating with others.\n",
    "\n",
    "Another benefit is *fair* model evaluation. Because pipelines ensure that every model is trained and validated using the same cross-validation splits — with consistent preprocessing inside each fold — the results are more directly comparable. This helps you make more accurate decisions when choosing between models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EMhY-TwVox4"
   },
   "source": [
    "### What have we learnt?\n",
    "Pipelines allow you to chain together data preprocessing and modelling steps into a single, unified workflow. This ensures consistency, helps avoid common mistakes like data leakage, and simplifies code management—especially when working with complex datasets like the Adult Income dataset, which contains both numerical and categorical features.\n",
    "\n",
    "We also learned how to extend pipelines with feature engineering, enabling us to reduce dimensionality and select the most informative features. When we integrate steps like imputation, encoding, scaling, and classification into one pipeline, we ensure that cross-validation and testing are fair and reproducible. Ultimately, pipelines provide a powerful, modular way to build, evaluate, and tune models in a clean and scalable manner—making them essential for any real-world machine learning project."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1mIc1vhK2-W4rJBFpJhxp3c7kI7nDEmuv",
     "timestamp": 1742822947450
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
